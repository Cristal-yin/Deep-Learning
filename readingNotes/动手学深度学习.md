[TOC]

# 动手学深度学习

## 深度学习简介

### 应用深度学习需要同时理解

+ 问题的动机和特点
+ 将大量不同类型神经网络层通过特定方式组合在一起的模型背后的数学原理
+ 在原始数据上拟合极复杂的深层模型的优化算法
+ 有效训练模型、避免数值计算陷阱以及充分利用硬件性能所需的工程技能
+ 为解决方案挑选合适的变量（超参数）组合的经验

### 学习环境

+ github  Jupyter  Sphinx  Discourse 

### 本书框架

![](../images/1.jpg)

![](../images/2.jpg)

![](../images/3.jpg)

## 预备知识

### 环境配置

### 数据操作

在MXNet中，NDArray是一个类，也是存储和变换数据的主要工具。

NDArray和NumPy的多位数组非常相似。

但是NDArray提供GPU计算和自动求梯度等更多功能。

+ 创建NDArray
+ 运算
+ 广播机制

### 自动求梯度



## 深度学习基础

### 线性回归

线性回归输出是一个连续值，因此使用与回归问题。比如，预测房屋价格、气温、销售额等连续值得问题。

与回归问题不同，分类问题中模型的最终输出是一个离散值。比如，图像分类、垃圾邮件识别、疾病监测等输出为离散值的问题都属于分类问题的范畴。

softmax回归则适用于分类问题。

线性回归和softmax回归都是单层神将网络，它们涉及的概念和技术同样适用于大多数数的深度学习模型。

+ 线性回归的基本要素

  举个例子：

  预测房屋价格的售出价格

  假设价格只取决于房屋状况的两个因素，即面积和房龄。

  + 模型

    房屋面积$x_1$ ,房龄$x_2$ , 售出价格 $y$ 建立模型

    $\hat{y} = x_1 w_1 + x_2 w_2 + b,$

    其中$w_1$ 和 $w_2$ 是权重， b是偏差， 且均为标量。它们是线性回归模型的参数。模型输出$\hat{y} $ 是线性回归对真实价格y 的预测或估计。通常允许有一定的误差。

  + 模型训练

    通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程就叫做模型训练。

    + 训练数据

      收集一系列真实的数据，在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。

      假设样本数为n，索引为i的样本的特征为$x_1^{(i)} $ 和 $x_2^{(i)}$  ,标签为$y^{(i)}$ 对于索引i的房屋，房屋价格预测表达式为

      $\hat{y}^{(i)} = x_1^{(i)} w_1 + x_2^{(i)} w_2 + b.$

      注： 

      收集的一组数据集叫训练数据集

      一栋房屋被称为一个样本

      真实售出价格叫作标签

      用来预测标签的两个因素叫作特征，特征用来表征样本的特点

      

    + 损失函数

      在训练中，我们需要衡量价格预测值与真实值之间的误差。通常会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数，它在评估索引为i 的样本误差的表达式为 

      $\ell^{(i)}(w_1, w_2, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,$

      其中常数1/2使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。给定训练数据集，这个误差至于模型参数相关，因此我们将它记为以模型参数为参数的函数。

      我们将衡量误差的函数成为损失函数，这里使用的平方误差函数也成为平方损失。

      通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量

      $\ell(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \ell^{(i)}(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2.$

      在模型训练中，我们希望找出一组模型参数，记为$w_1^*,w_2^*,b^*$ ,来使训练样本平均损失最小。

      $w_1^*, w_2^*, b^* = \operatorname*{argmin}_{w_1, w_2, b}\  \ell(w_1, w_2, b).$

    + 优化算法

      当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解。 

      本节使用的线性回归和平方误差刚好属于这个范畴。

      然而大多数深度学习模型并没有解析解，只能通过优化算法有限迭代模型参数来尽可能降低损失函数的值，这类解叫作数值解。

      在求解数值解得优化算法中，小批量随机梯度下降在深度学习中被广泛使用。

      **算法：**

      先选取一组模型参数的初始值，如随机选取；

      接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值；

      在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量$\mathcal{B}$ ；

      然后求小批量中数据样本的平均损失有关模型参数的导数(梯度)；

      最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

      在训练本节讨论的线性回归模型过程中，模型的每个参数将作如下迭代：

      $\begin{split}\begin{aligned} w_1 &\leftarrow w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_1} = w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &\leftarrow w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_2} = w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ b &\leftarrow b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial b} = b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}\end{split}$

      $|\mathcal{B}|$ 代表每个小批量中的样本个数，${\eta}$ 称为学习率并去正数。这两个参数都是人为设定的，因此叫作超参数。

      通常说的"调参"指的正是调节超参数。

  + 模型预测

    在优化算法停止时的$w_1, w_2, b$ 标记为 $ \hat{w_1}, \hat{w_2}, \hat{b}$ 。

    这里我们得到的并不一定是最小化损失函数的最优解 $w_1^*, w_2^*, b^*$  ,而是对最优解的一个近似。通过学出的线性回归模型$x_1 \hat{w}_1 + x_2 \hat{w}_2 + \hat{b}$ 来估算训练数据集意外任意一栋面积为x1,房龄为x2的房屋的价格，这里的估算也叫做模型预测、模型推断或模型测试。

+ 线性回归的表示方法

  + 神经网络图

    表示模型结构，神经网络图隐去了模型参数权重和偏差。

    ![*图 3.1* 线性回归是一个单层神经网络](http://zh.gluon.ai/_images/linreg.svg)

    线性回归是一个单层神经网络

    此神经网络中输入为x1 x2 ，输入个数叫特征数或特征向量维数。输出为$o$ ,输出层的输出个数为1.

    将途中神经网络的输出$o$ 作为线性回归的输出，即 $\hat{y} = o$ .

    由于输入层并不涉及计算，则神经网络的层数为1。

    所以，线性回归是一个单层神经网络。

    输出层中负责计算$o$ 的单元又叫神经元。

    $o$ 的计算依赖于$x_1 x_2$ 。

    也就是说，输出层中的神经元和输入层中各个输入完全连接。

    因此，这里的输出层又叫全连接层或稠密层。

  + 矢量计算表达式

    在模型训练或预测时，常常会同时处理多个数据样本并用到矢量计算。


### 线性回归从零开始

### softmax回归

线性回归模型适用于输出为连续值得情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。

+ 分类问题

  例如一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以yoga一个标量表示。我们将图像中的4像素分别记为$x_1 x_2 x_3 x_4 $ 。假设训练数据集中图像的真实标签为狗、猫或鸡 这些标签分别对应离散值$y_1 y_2 y_3$ 

  我们通常离散的数值来表示判别，例如 $y_1 = 1 y_2 = 2 y_3 = 3$ 。如此，一张图像的标签为1、2、3数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量，因此我们一般使用 更加适合离散值输出的模型来解决分类问题

+ softmax回归模型

  softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量偏差包含3个标量，且对每个输入计算$o_1 o_2 o_3$ 这3个输出

  $\begin{split}\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\ o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\ o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3. \end{aligned}\end{split}$

  ![softmaxåå½æ¯ä¸ä¸ªåå±ç¥ç"ç½ç"](http://zh.gluon.ai/_images/softmaxreg.svg)

  ​						图：softmax回归是一个单层神经网络

  上图用神经网络图描绘了上面的计算，softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$O_1 O_2 O_3$ 的计算都要依赖于所有的输入$x_1 x_2 x_3 x_4$ ，softmax回归的输出层也是一个全连接层

  + softmax的运算

    既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$ 当做预测类别是i 的置信度，并将值最大的输出所对应的类作为预测输出，即输出$atgmax_io_i$ 。例如，如果$o_1  o_2  o_3$ 分别为 0.1 10， 0.1 由于$o_2$ 最大，那么预测类别为2  其代表猫

    然而，直接使用输出层的输出有两个问题，一方面，由于输出层的输出值得范围不确定，我们难以直观上判断这些值的意义，例如，刚才举的例子中的输出值10表示"很置信"图像类别为猫，因为该输出值是其他两类的输出值的100倍，但如果$o_1 = o_3 = 10^3$ ,那么输出值10却又表示图像类别为猫的概率很低。另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量

    softmax运算符解决了以上两个问题，它通过下式将输出值变换成值为正且和为1 的概率分布 $\hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3),$

    其中 $\hat{y}_1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}_2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}_3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.$

    容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$ 且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$ ,因为$\hat{y}_1, \hat{y}_2, \hat{y}_3$  是一个合法的概率分布。 这时候 ，如果$\hat{y}_2=0.8$ ，不管$\hat{y_1} 和 \hat{y_2}$ 的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到

    $\operatorname*{argmax}_i o_i = \operatorname*{argmax}_i \hat y_i,$

    因此softmax运算不改变预测类别输出。

+ 单样本分类的矢量计算表达式

  为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为

  $\begin{split}\boldsymbol{W} = \begin{bmatrix}     w_{11} & w_{12} & w_{13} \\     w_{21} & w_{22} & w_{23} \\     w_{31} & w_{32} & w_{33} \\     w_{41} & w_{42} & w_{43} \end{bmatrix},\quad \boldsymbol{b} = \begin{bmatrix}     b_1 & b_2 & b_3 \end{bmatrix},\end{split}$

  设高和宽分别为2个像素的图像样本i的特征为

  $\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\end{bmatrix},$ 

  输出层为

  $\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\end{bmatrix},$

  预测为狗、猫或鸡的概率分布为

  $\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} & \hat{y}_2^{(i)} & \hat{y}_3^{(i)}\end{bmatrix}.$

  softmax回归对样本i分类的矢量计算表达式为

  $\begin{split}\begin{aligned} \boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}). \end{aligned}\end{split}$

+ 小批量样本分类的矢量计算表达式

  为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一小批量样本，起批量大小为n,输入个数为d，输出个数为q。设批量特征为$\boldsymbol{X} \in \mathbb{R}^{n \times d}$ 。假设softmax回归的权重和偏差参数分别为$\boldsymbol{W} \in \mathbb{R}^{d \times q}$ 和 $\boldsymbol{b} \in \mathbb{R}^{1 \times q}$ 。softmax回归的矢量计算表达式为 $\begin{split}\begin{aligned} \boldsymbol{O} &= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\ \boldsymbol{\hat{Y}} &= \text{softmax}(\boldsymbol{O}), \end{aligned}\end{split}$

  其中的加法运算使用了广播机制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$ 且这两个矩阵的第i行分别为样本i的输出$\boldsymbol{o}^{(i)}$ 和概率分布$\hat{y}^{(i)}$ 

+ 交叉熵损失函数

  前面提到，使用softmax运算后可以更方便地与离散标签计算误差。我们已经知道，softmax运算输出变换成一个合法的类别预测分布。实际上，真实标签也可以用类别分布表达：对于样本i，我们构造向量$\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}$ ,使其第$y^{( i)}$ 个元素为1，其余为0.这样我们的训练目标可以设为使预测概率分布$\boldsymbol{\hat y}^{(i)}$ 尽可能接近真实的标签概率分布$y^{( i)}$ 

  我们可以像线性回归那样使用平方损失函数$\|\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}\|^2/2$ 。然而，想要预测分类结果正确，我们其实并不需要宇哥概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$ ,那么我们只需要$\hat{y_3}^{(i)}$ 比其他两个预测值都要大就行了，即使$\hat{y_3}^{(i)}$ 值为0.6，不管其他两个预测值事多少，预测类别均正确。而平方素食则过于严格，例如$\hat y^{(i)}_1=\hat y^{(i)}_2=0.2$ 比$\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4$ 的损失要小很多，虽然两者都有相同正确的分类预测结果。

  改善上述问题的一个方法是使用更合适衡量两个概率分布差异的测量函数。其中，交叉熵是一个常用的衡量方法

  $H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},$

   其中带下标的$y_j^{(i)}$ 是向量$y^{(i)}$ 中非0 即1 的元素，需要注意将它与样本i类别的离散数值，即不带小标的$y^{(i)}$ 区分。在上式中，我们知道向量$y^{(i)}$ 中只有第$y^{(i)}$ 个元素$y^{(i)}_{y^{(i)}}$ 的值为1，其余全为0，于是$H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{y^{(i)}}^{(i)}$ 。也就是说，交叉熵值关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做到这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。

  假设训练数据集的样本数为n，交叉熵损失函数定义为

  $\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),$

  其中$\boldsymbol{\Theta}$ 代表模型参数。同样，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\ell(\boldsymbol{\Theta}) = -(1/n) \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}$ 。从另一个角度来看，我们只带最小化$\ell(\boldsymbol{\Theta})$ 等价于最大化$\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}$ ,即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。

+ 模型预测及评价

  在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别一致，说明这次预测是正确的。

+ 总结

  softmax回归适用于分类问题，它使用softmax运算输出类别的概率分布

  softmax回归是一个单层神经网络，输出格式等于分类问题中的类别个数

  交叉熵适合衡量两个概率分布的差异

### 图像分类数据集

在介绍softmax回归的实现前我们先引入一个多累图像分类数据集，它将在后面的章节中被多次使用，以方便我们观察鼻尖算法之间在模型精度和计算效率上的区别。图像分类数据集中最常用的是手写数字识别数据集MNIST[1] 。但大部分模型在MNIST上的分类精度都超过了95%。为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集Fashion-MNIST[2]

+ 获取数据集

  首先导入本节需要的包或模块

  

##   循环神经网络

### 语言模型

语言模型将计算该序列的概率    $P(w_1, w_2, \ldots, w_T).$

+ 语言模型的计算

  假设序列中的每一个词都是一次生成的

  $P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1}).$

  例如一个序列中含有四个词

  $P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3).$

  为了计算语言模型，我们需要计算词的概率，以及每一个词在给定前几个词的情况下的条件概率，即语言模型参数。

  如果训练数据集为一个大型文本语料库。词的概率可以通过该词在训练数据集中的相对词频来计算。

  例如，$P(w_1)$ 可以计算为$w_1$ 在训练数据集中的词频与训练数据集的总词数之比。

+ n元语法

  n元语法通过马尔可夫假设简化了语言模型的计算。马尔可夫假设是指一个词的出现只与前面n个词相关。

  当n比较小的时候，n元语法往往并不准确。当n比较大的时候，n元语法需要计算并存储大量的词频和多次相邻频率

  那么如何才能平衡两点呢？

### 循环神经网络

循环神经网络并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。

+ 不含隐藏状态的神经网络

  **单隐藏层的多感知器**

  给定样本数为$n$ 、输入个数为$d$  的小批量数据样本  $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ 

  设隐藏层的激活函数为 $\phi$  ,那么隐藏层的输出   $\boldsymbol{H} \in \mathbb{R}^{n \times h}$ 

  计算为    $\boldsymbol{H} = \phi(\boldsymbol{X} \boldsymbol{W}_{xh} + \boldsymbol{b}_h),$

##  自然语言处理

### 词嵌入

词嵌入：把词映射为实数域向量的技术

+ **为何不用one-hot向量 **

  